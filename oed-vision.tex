% here is the DOI:
% 10.6084/m9.figshare.5339896
%--------------------------------------------------------------%

\documentclass[11pt]{article}

%% \usepackage{times}
% \usepackage{newcent}
%% FONTS
%% To get the default sans serif font in latex, uncomment following
%% line:
%% Note: the default sf font is much nicer than ariel/helvetica
 \renewcommand*\familydefault{\sfdefault}
%%
%% to get Arial font as the sans serif font, uncomment following line:
% \renewcommand{\sfdefault}{phv} % phv is the Arial font
%%
%% to get Helvetica font as the sans serif font, uncomment following line:
% \usepackage{helvet}
 \usepackage{wrapfig}
%% NATBIB is the one to use here. Just be careful since it puts space
%% between references; there's probably some customization that
%% compresses the space out (like the cite package does).
%% For CDI, space was tight so we went back to cite2
% \usepackage[square,numbers,sort&compress]{natbib}
%% note that hyperref has problems with the cite package--better to
%% use natbib
% \usepackage[sort,nocompress]{cite}
 \usepackage[sort,compress]{cite}
\usepackage[margin=0pt,labelsep=space,footnotesize,labelfont=bf,up,belowskip=-10pt,aboveskip=5pt]{caption}
%\usepackage[small,bf,up]{caption}
\renewcommand{\captionfont}{\footnotesize}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphics,epsfig,graphicx,float,subfigure,color}
\usepackage{algorithm,algorithmic}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{multicol}
% \usepackage{subsec}
\usepackage{comment}
\usepackage{array}
\usepackage{marvosym}
\usepackage{url}
\usepackage{boxedminipage}
 \usepackage[sf,bf,small]{titlesec}
% \usepackage[sf,bf,small,compact]{titlesec}
% \usepackage[textsize=footnotesize]{todonotes}
 \usepackage[plainpages=false, colorlinks=true,
   citecolor=blue, filecolor=black, linkcolor=blue,
   urlcolor=blue]{hyperref}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{pdfpages}
\usepackage{paralist}
\usepackage{enumitem}
\usepackage{doi}
% \usepackage{showkeys}
 \usepackage[bottom]{footmisc}

%\newcommand{\gbIn}[1]{\textcolor{magenta}{#1}}
\newcommand{\gbIn}[1]{{#1}}
\newcommand{\gbOut}[1]{}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
% see documentation for titlesec package
% \titleformat{\section}{\large \sffamily \bfseries}
\titlelabel{\thetitle.\,\,\,}
% \titlespacing*{\section}{0pt}{2ex}{0.5ex}
% \titlespacing*{\subsection}{0pt}{1.5ex}{0.25ex}
%% \titlespacing*{\subsubsection}{0pt}{1ex}{0ex}

% \renewcommand{\baselinestretch}{0.984}

\newcommand{\gbf}[1]{\text{\boldmath${#1}$\unboldmath}}
\newcommand{\bs}{\boldsymbol}

\newcommand{\edot}{\dot{\gbf{\varepsilon}}}
\newcommand{\secinve}{\edot_\mathrm{II}}
\newcommand{\secinvt}{\gbf{\tau}_\mathrm{II}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cR}{\mathcal{R}}

\newcommand{\obs}{\mathrm{obs}}

\newcommand{\bit}{\begin{itemize}}
\newcommand{\eit}{\end{itemize}}

\newcommand{\bdm}{\begin{displaymath}}
\newcommand{\edm}{\end{displaymath}}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
% \newcommand{\alert}[1]{\textcolor{SkyBlue3}{#1}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}

\newcommand{\tcred}[1]{\textcolor{red}{#1}}

\newcommand{\mcone}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\tentwo}[1] {\ensuremath{\boldsymbol{#1}}}
\newcommand{\tenfour}[1] {\ensuremath{\boldsymbol{\mathsf{#1}}}}
\renewcommand{\vec}[1] {\ensuremath{\boldsymbol{#1}}}

\newcommand{\zapspace}{\topsep=0pt\partopsep=0pt\itemsep=0pt\parskip=0pt}

\newcommand{\footnoteremember}[2]{\footnote{#2}\newcounter{#1}\setcounter{#1}{\value{footnote}}}

\newcommand{\footnoterecall}[1]{\footnotemark[\value{#1}]}

%\newcommand{\AdM}{additive manufacturing}
\newcommand{\AdM}{AM}



\definecolor{darkred}{rgb}{.6,.1,.1}
\definecolor{darkblue}{rgb}{.1,.1,.9}
\definecolor{grass}{rgb}{.19,.64,.13}
\definecolor{darkgreen}{RGB}{0,170,0}


\newcommand{\app}{\textcolor{darkred}}
\newcommand{\thrust}{\textcolor{darkblue}}
\newcommand{\theme}{\textcolor{red}}
\newcommand{\resthrust}{\textcolor{darkgreen}}

\newcommand{\AM}{AM } % AM = applied math
\newcommand{\CSE}{CS\&E}
\setlength{\emergencystretch}{20pt}

\addtolength{\skip\footins}{-5pt}

\begin{document}


\begin{center}
{\large \textbf{ A Unified Mathematical, Computational, and
    Experimental Approach for Integrating Data and Models}}
\end{center}
%

\section{Overview}

The scientific method entails the systematic acquisition of knowledge
about our world via the continuous interplay of theory, computation
and experiment---that is, of models and data. The rapid ascendance of
high performance computing has radically transformed our ability to
model complex multiscale systems and to analyze complex multimodal
data. A recent surge of interest in machine learning has brought
renewed emphasis to the opportunities that lie in learning from data,
yet the robust and rigorous application of machine learning in the
scientific context remains in its infancy.

To capitalize on these advances in modeling capabilities and on DOE's
considerable investment in experimental facilities, there is a
critical need for {\em a principled, rigorous, and scalable
  mathematical approach to optimally guide the interplay between
  complex models and complex data---and to account for uncertainty in
  the process}.  In current practices, the methodologies by which
experiments inform theory, and theory guides experiments, remain ad
hoc, particularly when the physical systems under study are
multiscale, large-scale, and complex.  Off-the-shelf machine learning
methods are not the answer---these methods have been successful in
problems for which massive amounts of data are available and for which
a predictive capability does not rely upon the constraints of physical
laws.  The need to address this fundamental problem has become urgent,
as computational science attempts to tackle models that span wider
ranges of scales, represent richer interacting physics, and inform
decisions of greater societal consequence.

Here we describe a research program (initially leveraging Brookhaven National Laboratory's experimental user facilities)
aimed at developing {\em a
  unified approach (mathematical, computational, and experimental) for
  the systematic integration of complex multimodal data and complex
  multiscale models via physics-based inference, scientific machine learning,
  and goal-oriented optimal experimental design.}  The interaction
between models and data occurs in two directions:
 \vspace{-0.15cm}
 \begin{itemize}%[leftmargin=10pt]
 \zapspace
 \item The problem of how data can be used to inform models is
   fundamentally an {\bf inverse problem}. While inverse theory has a
   long history, only in recent years has it become tractable to
   rigorously address inverse problems under uncertainty. Bayesian
   inverse theory provides a rational and systematic approach for
   learning from data through the lens of models under both data and
   model uncertainty, producing a probability distribution as the
   inverse solution.  However, in the context of large-scale complex
   models, numerous challenges must be overcome, including the
   multiscale and multiphysics nature of the models, the high
   dimensionality and heterogeneity of parameter space, the
   availability of multiple competing models and their structural
   uncertainties, the multimodality and complexity of data (which can
   stem from experiments, observations, and simulations), and the need
   to incorporate complex nonlinear constraints into priors that
   inform inference and machine learning algorithms.

\item How, where, when, and from which source to acquire experimental,
  observational, or simulation data to optimally inform models
  with respect to a particular goal or goals is fundamentally an {\bf
    optimal experimental design (OED) problem}. Probability provides a
  powerful approach for addressing these problems: since the inverse
  problem solution is equipped with quantified uncertainties, the OED
  problem naturally seeks to design experiments to minimize the
  uncertainty in predictions of interest.  Thus, the OED problem
  inherits all of the challenges for inverse problems described above,
  including model and data complexity and high
  dimensionality. Moreover, because solutions of nonlinear inverse
  problems depend on the data, the inverse problem is formally
  embedded as a constraint within the OED problem, rendering the
  latter prohibitive using conventional methods.

\end{itemize}

% {\bf Applications.}
% The set of methods
  The framework described above can be applied to a host of problems
  of interest to the Department of Energy.  To make the above ideas
  concrete, however, we propose to develop the necessary mathematical
  and computational aspects of this approach in the context materials science, biology, and nuclear physics
% This includes (1) autonomous
% optimal design of experiments for synthesis-by-assembly and (2)
% combining machine learning with dynamical mean field theory for
% strongly correlated materials discovery.  In (1) the challenge is to
%  use multiscale models integrated with the synthesis capabilities at
%  the Center for Functional Nanomaterials to design autonomously and
%  optimally synthesize new materials not through conventional
%  small-molecule chemical synthesis, but instead by
%  mixing-and-matching nanoscale components.  In (2) the challenge is
%  is how to enable the discovery of new materials with targeted
%  functionalities in strongly correlated systems. 
These applications span the space of the types of Department of Energy BES
  experimental user facilities and stress-test the capabilities with
  challenging problems.  The tools should be broadly applicable beyond
  the applications here.

It is clear that creating the mathematical approach described
above---in which models optimally learn from data and data acquisition
is optimally guided by models---presents mathematical and
computational challenges of the highest order when the systems of
interest are complex, multiscale, strongly interacting/correlated, {\em and}
uncertain. But these challenges must be overcome to realize the
promise of predictive science: experiments and high-resolution
simulations are too expensive for experimental design to be conducted
in an ad hoc fashion and for the resulting data not to be exploited to
its very fullest.

We argue that the key to overcoming these mathematical and
computational challenges is to exploit the mathematical structure of
the inverse/learning and OED problems, for example the nonlinearity,
smoothness, sparsity, low dimensionality, and hierarchical structure
of the maps from inversion parameters to observables (for the inverse
problem) and from experimental design variables to posterior
uncertainties (for the OED problem).  Novel machine learning methods
that build in as much physics knowledge as possible will be required.
A critical issue is that methods that view these maps as black boxes
cannot efficiently exploit the structure of the inverse and OED
problems. Instead, intrusive algorithms that ``open up the black box''
must be developed, building on advances in Bayesian methods,
uncertainty quantification, randomized algorithms, low-rank
approximation, hierarchical matrices, model reduction, manifold
learning, PDE-constrained optimization, high-dimensional approximation
theory, higher-order adjoint methods, tensor methods, parallel
algorithms, and others.

Significant computational and mathematical work in this direction has
been undertaken by the DiaMonD MMICC Center funded by ASCR. The
resulting algorithms have been applied to Bayesian inverse and
stochastic optimization problems involving complex applications with
as many as $10^6$ parameters.  Algorithmic complexity (measured in the
number of forward model solves) has been demonstrated to be
independent of parameter, data, and optimization variable dimensions.
However, significant work remains to overcome the difficulties posed
by the most challenging multiscale, multiphysics DOE application
problems.  Moreover, BNL's expertise in workflow design, and complex
modeling and machine learning frameworks integrated with BNL's experimental
facilities can be leveraged.  Key to success also requires continued
development of theory and models for the systems of interest.

\section{Materials Science Challenges}

\subsection{Synthesis by Assembly}

The grand challenge of modern materials science is the rational design
of new materials, where given a desired material functionality, the
material structure is predicted; and for that particular structure,
appropriate constituents and assembly processes are
designed. Synchrotron x-ray scattering plays an essential role in
unraveling these relationships, by providing a powerful tool to probe
the structure of materials in situ. With the needs for material
functionality becoming more diverse, stringent, and sophisticated, the
complexity of materials continues to increase. The relevant parameter
spaces expand correspondingly, arising from both the multi-component
nature of functional materials and a multitude of processing
conditions. All of this implies that optimizing functionality requires
strategic exploration of the vast parameter and model structure space
that is associated with complex materials. To meet this challenge, the
way we investigate materials structure by x-ray scattering needs to
evolve, to become more efficient and intelligent.  We propose to build
towards a new paradigm of scientific discovery, where automatable
tasks are ceded to machine control, and human experts are liberated to
work on the challenging high level problems of truly understanding and
applying materials science. Specifically, our goal is to implement a
prototype autonomous x-ray scattering instrument at the Complex
Materials Scattering beamline (CMS/11-BM) at National Synchrotron
Light Source II (NSLS-II). We have already been making rapid progress
with automating the beamline data collection workflow and developing
data analysis pipelines. A key missing component is autonomous
experimental decision-making. This initiative will develop an online
experimental control system that leverages both real-time data
analysis and materials theory to make optimal experimental
choices. Autonomous decision making will enable materials discovery of
a speed and scope previously unattainable.

Our approach leverages novel methods in goal-oriented Bayesian optimal
experimental design.  In a nutshell, we cast the problem of selecting
the next experiment (sample, processing conditions, measurement
parameters, etc.) as an optimization problem under uncertainty. The
quantity to be optimized can be tuned to the materials system and the
particular problem under study. For instance, one can design a target
metric to maximize the coverage of a parameter space to enable
intelligent mapping, to maximize rigorously-defined metrics of
surprise and creativity in order to emphasize novel discovery, or to
maximize a targeted material structure in order to autonomously
discover an optimized synthesis protocol.

The CMS beamline is well positioned to tackle this challenging
project, because most of the components necessary for autonomous x-ray
scattering experiments are either in place or in an advanced stage of
development. Routine operations at CMS already utilize a series of
automated data collection steps, and their versatility is continually
being enhanced through improvements in software codes. Moreover, we
have recently made significant progress with expanding accessible
sample parameter spaces by implementing several in-situ sample
environments and installing a robotic sample exchanger to increase
throughput. Finally, the ongoing development of real-time data
analysis pipelines and classification for x-ray scattering images is
now mature enough for initial deployment and testing at
CMS. Nevertheless, these developments alone are insufficient, allowing
for parameter-space explorations that are only exhaustive or
intuition-guided. By closing the feedback loop with an automated
decision-making capability that is well informed by available
knowledge, this project will enable autonomous experiments that can
navigate intelligently through enormous parameter spaces.  Overall,
this project will empower a bold new vision of materials discovery,
wherein scientists can define their scientific problem at a high
level, as an optimization target, and allow the x-ray scattering
instrument to autonomously discover relevant physics. We will focus on
solving specific experimental materials science problems, so that we
can (1) demonstrate proof-of-concept (as a basis for future funding),
(2) develop new generalizable algorithms for autonomous
experimentation, and (3) uncover new physical insights for the
selected materials systems. This work will establish a capability at
BNL for model-based control and design of experiments to accelerate
discovery at experimental facilities. The novelty of the computational
science and applied mathematics will be in the development of accurate
and scalable approximate methods for optimal experimental design
problems under uncertainty.
% that scale well with computational resources.

\subsection{Strongly Correlated Systems}

One of the most challenging questions in the field of materials
science is how to enable the discovery of new materials with targeted
functionalities. Materials of interest range over binary, ternary,
quaternary, and quinary combinations of a wide range of elements,
rendering the material phase space immense.  Multiple strategies do
exist to attempt to classify this space.  In one, one uses massive
simulations in an attempt to explore the phase space for particular
properties. In another, one uses data mining of large bodies of
existing experimental data in order to determine structure-property
relations.  A more recent approach that has been applied recently to
weakly correlated materials such as zeolite structures,
inorganic/organic hybrids, and semiconducting heterostructures is
machine learning.  Machine learning algorithms allow one to perform
predictive analytics based on the detection of patterns and
correlations in large datasets. We will be particularly interested in
active learning algorithms, machine learning algorithms that interact
in real time with companion algorithms generating the
datasets. Ideally in an active learning setting, the machine learning
algorithm populates the dataset in a maximally efficient manner so
that its needed size is minimized. We will also employ transfer
learning, a methodology where we will iteratively improve an existing
machine learning model using smaller, but higher quality, datasets.
Machine learning-informed investigation of weakly correlated material
spaces has seen some progress, both because theoretical tools exist to
accurately predict material properties for weakly correlated materials
and because high-throughput synthesis and characterization is possible
In weakly correlated materials, we have
a set of well-developed theoretical tools such as Density Functional
Theory (DFT) that are able to describe their properties. DFT is able
to do so because the properties of these materials are determined in a
single electron picture, i.e., electronic band theory is
correspondingly applicable.

While weakly correlated materials have been studied using machine
learning, the field of strongly correlated materials has lain
untouched by such approaches. This is unfortunate. Correlated
materials have exceptional properties ranging from metal to insulator
transitions, colossal magnetoresistance, high temperature
superconductivity, heavy fermion behavior, and huge volume collapses,
to name but a few.  Heretofore, discoveries in this field of research
have been made mostly by serendipity. There are very few examples
where a strongly correlated material was predicted to exist with a
certain functionality before it was synthesized and characterized
experimentally. A near miss was the prediction of superconductivity in
the 112-pnictides BaFeAs2 and BaFeSb3, whereas superconductivity
was only later shown to exist in the related 112-compound
Ca1-xLaxFeAs2. It is an aim of this project to improve on this
record.  The absence of machine learning applied to strongly
correlated materials is a result of the difficulty of creating a high
quality database on which a machine learning algorithm can be
trained. \textcolor{red}{Our approach is based on the central idea
  that creating a training database is fundamentally a problem in
  optimal experimental design.} In part this difficulty arises from
the attendant theoretical challenge. The description of even a single
point in strongly correlated materials space is difficult. Strongly
correlated materials are in general not well described by band theory,
thus rendering DFT inapplicable. Moreover strongly correlated
materials typically see competing types of order (charge density wave
formation versus superconductivity) whose energies are very close to
one another. To determine which prevails requires theoretical
treatments that are highly accurate. Experimentally, high throughput
synthesis is a challenge for these same reasons. The synthesis of high
quality samples of a given strongly correlated material is difficult
and can take years to perfect.

The overarching objective of this project is to overcome the
roadblocks that machine learning aided discovery of new strongly
correlated materials face. While we will target oxide heterostructures
our success will depend on us meeting four more general objectives: (i)
To develop and extend the tools, theoretical and algorithmic, based on
dynamical mean field theory, so that they operate in an HPC
environment to enable materials discovery via machine learning
algorithms. (ii) To employ active learning methods combined with
transfer learning techniques in an HPC environment to better
guide materials discovery. (iii) To import physics information into
machine learning algorithms to improve their performance. (iv) To
perform detailed analysis, both theoretical and experimental, of
materials identified by our machine learning algorithms as having
targeted functionalities. Experimentally we will attempt both
synthesis of the predicted materials using state-of-the-art molecular
beam epitaxy techniques together with characterization using the
beamlines at BNL's NSLS-II.

\section{Biological Science Challenges}
\subsection{Systems Biology for Bioenergy Production}
The long-term vision of Biological Systems Science is to enable predictive biology. To meet this goal, it has been recognized that biological complexity must be both embraced and leveraged. Key will be determining each of the genome-encoded factors contributing to biochemical pathways of interest, their specific molecular function and how they interact. Understanding these interactions in sufficient detail to allow system behavior to be predicted, even in response to significant environmental perturbations, will be crucial for robust genome-scale redesign of bioenergy. 
As molecular biology has transitioned to systems biology in the post-genomic era, the adoption of in silico approaches has become essential in data-centric modern biology. One manifestation of this movement has been the development of constraint based models of cellular metabolism, wherein the cellular, stoichiometric and enzyme reaction limits are defined, and all allowable solutions may be searched. Within defined space, the model may be optimized to allow for optimal production of for example a key metabolite of specific interest, or enable optimal cell viability when presented with limited nutrients. However, for the aforementioned reasons these models are far from complete. Only a small fraction of any given genomes encoded enzymes are sufficiently characterized to come close to mapping a model representative of a complete cell. Very few models are sufficiently sophisticated to incorporate individual cellular spaces and the kinetics of trafficking metabolites between these spaces. Instead, pathways unique to mitochondria or chloroplasts in plant cells are often assumed to also occur in the cytoplasm. Metabolic models also suffer from being informed by data gained from a handful of model organisms (e.g. Escherichia coli, or Saccharomyces cerevisae), which are generically assumed to be representative of very distantly related organism.
The current absence of the foundational knowledge surrounding protein function, regulation and interaction serves as a barrier for model improvement and consequently the ultimate goal of biological predictive design. This comes in spite of a series of significant recent milestones in biology: i) every organism genome (or ecosystem) is amenable to genomic and post-genomic (e.g. RNA-Seq) analysis, ii) access and availability of this genome-sourced data is democratized across in silico platforms iii) gene synthesis is both rapid and cost-effective iv) the development of techniques enabling genome-scale precision editing. Consequently, and paradoxically, molecular biologists are faced with being able to engineer biological systems genome-wide, but lacking the insight of what or how to optimally rewire metabolism for designed gain. Given this, many engineering projects have met with limited success: knowing what to engineer and how to engineer it to achieve a desired outcome remains a bottleneck. This is especially the case in plants, which, due to genome size, ploidy extensive genetic radiation and until recently limited genetic tools, have remained far less intensively studied than other domains of life.
We have established the Quantitative Plant Science Initiative (QPSI) at BNL as a scalable capability combining multi-disciplinary expertise and state-of-the-art high throughput (HTP) technologies. Specifically, the purpose of this capability is to accelerate the acquisition of systems-level functional knowledge that will enable predictive plant biology. One core component of the capability exploits the rapid growth rate and genetic tractability of microbial photosynthetic organisms, which maintain most of protein-coding repertoire of more complex land plants, in combination with laboratory automation. Via experimental miniaturization we are able to use these microbial plants to generate near-genome-saturating mutant libraries in approximately 2 hrs by high-efficiency transformation with a selectable gene cassette marker. The marker integrates at random in the genome and disrupts the function of any gene into which it inserts. Robot-enabled mutant picking, arraying, and compression into 384-well microplates is followed by automated simultaneous screening of 30,000 single-gene disrupted lines under defined growth conditions, with each screening cycle taking around 5 days to complete. This experimental setup enables us to probe at a genome-scale level all genes involved in any process we choose to screen for.
Having identified genes of previously unknown function in the process described above, a second core component of QPSI involves the engineering of proteins for improved performance. This is done by both rational design methods, for example introducing mutations into the active site of an enzyme, or by non-targeted directed evolution methods. The mutated proteins resulting from either of these approaches are screened for activity and those resulting in the greatest improvement can be engineered into the organism’s genome.
Both of these components serve to gain from application of optimal experimental design. Since our phenome screening experiments can be either independent or iterative, being provisioned with guidance as to the next most appropriate screen, or which collection of genes next to target, towards a defined end goal may result in significant time and reagent expenditure. Secondly, it is highly likely that our engineering will yield multiple single nucleotide polymorphism (SNP) mutations that result in higher activity. Knowing specifically which combinations of these mutations to reconstitute on the genome for the greatest overall gain is both protein-specific and largely unknown. Intuitively, and in the possible absence of information (such as a crystal structure), reengineering a handful of SNPs with the greatest individual activity increase will lead to the greatest overall increase. In reality it is unknown how these SNPs will interact and/or influence the three-dimensional shape. Exploiting OED to learn from iterative rounds of genome engineering efforts and future guidance for selecting mutations to carry forwards will potentially lead to increased understanding of how to engineer proteins as well as expediting a route to optimized improvements. Finally, development and implementation of the underlying math and computation for these specific cases will major positive outcomes in other areas of biology and healthcare.  

\subsection{Adaptive Free Energy of Binding of Small Molecules}

The strength of drug binding is determined by a thermodynamic property known
as the binding free energy (BFE). One promising technology for estimating
binding free energies and the influence of protein and ligand composition upon
them is molecular dynamics (MD) A diversity of methodologies have been
developed to calculate binding affinities MD sampling and blind tests show
that many have considerable predictive potential. With the demands of clinical
decision support and drug design applications in mind, several computational
protocols to compute BFE that have been designed recently. To name just a
couple: ESMACS (enhanced sampling of molecular dynamics with approximation of
continuum solvent) and TIES (thermodynamic integration with enhanced
sampling).

BFE protocols also address the hitherto lack of reproducibility of individual
simulations for a variety of protein systems. The reproducibility of these
protocols is statistical, in that it overcomes the lack of accuracy of single
simulations, by employing ensemble-based molecular dynamics, where an
``ensemble'' refers to the set of replicas. Averaging across multiple runs can
reliably produce results in agreement with previously published experimental
findings.


\noindent {\bf Importance of Adaptivity for BFE Protocols:} An adaptive application
workflow is defined as one for which the execution task graph changes during
the runtime of the application workflow. The task graph could evolve due to
any one of several different factors. For example, a change in the underlying
protocol (algorithm), a change in the resource performance characterization,
or an event of interest (e.g., convergence critieria reached or lack of
convergence in a given window).

A driver for adaptivity in BFE calculations is that different protocols
(algorithmic methods) typically involve compounds with a wide range of
chemical properties which can impact not only the time to convergence, but the
type of sampling required to gain accurate results. In general, there is no
way to know before running calculations exactly which setup of calculation is
required for a particular system. BFE are computationally expensive; given the
very large number of drug candidates, it is imperative to optimize the
execution time while still improving the accuracy of candidate compounds.
Adaptive methods which minimize the compute time used whilst producing binding
free energy estimates meeting pre-defined quality criteria (such as
convergence or statistical uncertainty below a given theshold).

% BFE protocols have been sucessfully used to predict binding affinities quickly
% and accurately. Nonetheless, they are very expensive computationally, and
% optimizing the execution time while still improving the accuracy is necessary.
% Given the very large number of drug candidates, it is imperative to gain
% maximum insight into potential candidate compounds using time and resources
% efficiently. This provides one clear motivation for the use of adaptive
% methods which minimize the compute time used whilst producing binding free
% energy estimates meeting pre-defined quality criteria (such as convergence or
% statistical uncertainty below a given theshold).
  
In protocols such as TIES or ESMACS the number of ensemble members that will
most impact the calculation are not known \textit{a priori}, and change
between physical systems (drugs). Adaptive placement of $\lambda$ windows is
likely to better capture the shape of the $\partial U/\partial\lambda$ curve,
leading to more accurate and precise results for a given computational cost,
as opposed to sampling each window with high frequency, leading to more
accurate and precise results for a given computational cost. On occasion,
alchemical methods may be very slow to converge; in such circumstances use of
another method, such as ESMACS, may be the best option. This means that the
most effective way to gain accurate and precise free energy results on
industrially or clinically relavant timescales is to be able to adapt both
sampling (intra-protocol) and even the type of calculation (inter-protocol)
used at run time. With potentially thousands of simulations, often employing
multiple analysis methodologies, this provides the most effective way to
utilize these techniques and resources at scale.

\noindent {\bf Mathematical Formulation of the Adaptive Execution: } We believe the
problem of determining an optimal execution strategy for concurrent stochastic
simulations can be formulated as:

Imagine there are N resources and these can be run in parallel. There are M
different stochastic algorithms which calculate the same quantity of interest
but with different variances for the same amount of compute time. As a trivial
example you have M different ways to calculate the average of some function
say.  In the limit of an infinite run on any computing resource, the variance
will be zero.  

The distribution of run times for a given algorithm and a prescribed variance
($\sigma$) on computing resource $i$, P$_i$($\sigma$,t) i=1,...,M. A priori we
don’t know this distribution but we can learn about it from runs. We have a
fixed amount of overall time to run the jobs (which can be run in parallel)
and a fixed N. One task might be to find the optimal selection of algorithms
to, for fixed T and given N, we can minimize the variance of the estimate of
the mean.  The objective is to determine a strategy to determine how to
optimize the execution. The advantages of determining optimal execution
strategies include: (i) Greater sampling and higher throughput of drug
candidates; (ii) more accurate binding affinity calculations, and (iii)
Efficient resource utilization.

\noindent {\bf Challenges of Adaptive Execution: } The phase space of possible
execution strategies is large and is sensitive to algorithm (BFE protocol),
size of ensemble, and must be computed in real-time. Furthermore, there are
different metrics for which optimal solutions might be desired. For example,
some times an optimal strategy for different drug candidates to determine the
maximal convergence for a given (fixed) amount of computing capacity
(resources) is required. Some times, for a fixed level of convergence, optimal
utilization of computational resources are needed.

% The sampling of each drug candidate varies; thus using a static and 
% pre-determined computational time, results in a large variation in the accuracy of
% the BFE. Further, the performance of resources fluctuate: planning for
% resource utilization against a backdrop of varying resource performance and
% application-level changes.

% (i) Determining accurately the amount/degree of sampling, (ii)  Resource
% Selection and Utilization problem


\section{Nuclear Physics Challenges}
 
One of the top priorities of DOE's Office of Science Nuclear Physics
program is to understand the high temperature quark-gluon plasma (QGP)
state.  The QGP is the novel state of matter through which the
universe evolved only microseconds following the big bang.  In this
regime the unbound quarks and gluons move coherently in a nearly
perfect (lowest theoretical viscosity) liquid like-state.
Understanding the nature of this exciting state of matter and how it
is created is one of the great challenges of QCD physics.  The
Relativistic Heavy Ion Collider (RHIC) at BNL has created a QGP, and 
has begun to explore the engineering of its properties, by varying 
the beam energy and species of the colliding nuclei that we use to 
form the QGP, and by taking advantage of the natural
variation in the initial conditions for its formation.  As a
result of the time and spatial scales involved, however, one cannot
directly observe the QGP state and its nature must be determined by
marrying experimental data of a later stage in the process with
uncertain physics models of the dynamics.  This requires
multiscale/multiphysics and Bayesian inversion methods to infer the
unseen state.  

 With the diversity
and complexity of the physical processes involved, mathematical
challenges abound in inferring the QGP dynamics.  First the spatial
and time scales range from $10^{-15}$ meters to $10^{1}$ m.  For the
first $10^{-23}$ seconds after formation the QGP dynamics has a
continuum description in terms of relativistic hydrodynamics.
However, this early stage is not directly observable.  As time
evolves, the density decreases and a kinetic stage kicks in which is
governed by a Boltzmann equation.  The system evolves from a QGP
phase, hadronizes into a gas phase, and then emerges into more
familiar particles at the detector.  Inference of the early
hydrodynamic stage requires solving inverse problems that sequentially
couple hydrodynamics and kinetics.  The data from the experiment is
noisy, and despite a deep understanding of many of the physical
processes there remains considerable uncertainty in the form and
parameters of the hydrodynamic and kinetic descriptions.  This
uncertainty necessitates a computationally intensive iterative process
for determining the hydrodynamic state and initial conditions.
Bayesian methods are typically used and it is expensive to compute an
ensemble of the forward models evolutions.  Model reduction and an
optimal design of computer experiments (as well as actual accelerator
/ experimental design) is needed for a more complete understanding of
QGPs. To date, the problem has been simplified to one of estimating
$O(10)$ parameters for the model and for the initial
condition \cite{bernhard2016applying, bernhard2015quantifying,
auvinen2016systematic, bernhard2017characterization,
bass2017determination}.  We will attempt to incorporate greater model
uncertainty and to invert for more of the physical state at various
times during the experiment.

We propose to take a 3-part strategy to applying the tools
developed by this research towards the study of the QGP.
In the first stage, we will explore an optimization of the 
upcoming Beam Energy Scan program beginning in FY19.
In the Beam Energy Scan program, RHIC will 
explore the QCD phase diagram by changing the baryon doping 
of the QGP, enabled by changing the beam energy of
the colliding ions, in order to find, or exclude, a conjectured
critical point.  The measures sensitive to the critical point
are well understood, as are the costs and benefits of running
at a certain beam energy.  The question to be answered is 
what combination of beam energies, with what amount of
running time, would maximize the potential of the critical
point search.

In the second stage, we will explore the optimization
of the allocation of data bandwidth within the experiment.
All collider experiments cannot record full event information
for every collision that occurs in the collider, but use a combination
of partial event information and fast processing (usually in FPGA's)
to tag in real time (at multi-MHz speed) those events
that are most interesting for later analysis.  This process is 
called ``triggering'', with the result that the limited bandwidth
of full events written to the computer farm for later analysis
is filled by a suite of different ``triggers'', up to 64 in the 
case of the STAR experiment, each of which is targeted to
a specific physics program, with its own false positive
rate and efficiency for selecting the events of interest.
 The relative population of triggers are controllable, 
via trigger definition and random
sampling, and change with time as the conditions in the 
collider change.  The question to be answered is the optimization
of this trigger population to maximize the physics potential, in real time,
given the constraints and conditions of the collider and experiment.

In the third stage, we will extend these techniques to 
higher dimensional hydrodynamic modeling.  Besides
the search for the critical point, a wider goal of the Beam Energy
Scan is to explore how the hydrodynamic properties
of the QGP, such as the viscosity, change with baryon 
doping and initial temperature, in order to take 
full advantage of the three orders of magnitude span
in available collision energy with the combined data from RHIC
and the LHC.   

 In addition to providing higher confidence in the
description of QGP physics, this motivating application provides a
representative testbed for a set of experiments where no direct
observation is possible but only an inversion of multiscale and
multiphysics processes are available only at later times.  Such
applications appear in a host of other exciting and important
dynamical inverse problems including earth science, climate, medical
imaging. The methods developed here could benefit optimal use of large
experimental facilities as well as in their design.  Moreover, BNL is
in the process of developing the experimental program for a future
Electron-Ion Collider (EIC) collider at BNL (eRHIC). The design
considerations of the experiments are derived from the detection of
the different signatures. Each signature carries its own experimental
challenges and include feedback to how well one can constrain the
theoretical predictions.  The optimization of the eRHIC design and
experiments could make use of the mathematical and algorithmic methods
developed in Diamond2.  This is important for the RHIC program, e.g.,
identifying the optimal set of beam energies for the beam energy scan
to search for a critical point, and to select optimal collision
systems by asking the question which data would be most impactful with
respect to answering specific physics questions.  The methods here
could also provide a breakthrough for the design of optimal detector
configurations (acceptance, solution, etc.) for future EIC
experiments.




\section{Software System for High-Performance Optimal Experimental Design}

\section{Concluding Remarks}

\newpage



\begin{center}
CONTRIBUTORS (SO FAR)
\normalsize Francis Alexander, Ian Blaby, Masafumi Fukuto, Nicholas D'Imperio, Adolfy Hoisie,
Shantenu Jha, Kerstin Kleese van Dam, Robert Konik, Gabi Kotliar,
Kevin Yager, Shinjae Yoo (Brookhaven National Laboratory), Omar
Ghattas, J. Tinsley Oden (UT Austin), Karen Willcox, Youssef Marzouk
(MIT), Edward R. Dougherty, Xiaoning Qian (Texas A\&M), Kristofer
Reyes (Buffalo), Lav Varshney (Univ. of Illinois)

\end{center}


\end{document}


